{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def summation_unit(inputs, weights):\n",
        "    \"\"\"\n",
        "    Calculates the weighted sum of inputs.\n",
        "\n",
        "    Args:\n",
        "        inputs: A list or NumPy array of input values.\n",
        "        weights: A list or NumPy array of corresponding weights.\n",
        "\n",
        "    Returns:\n",
        "        The weighted sum of inputs.\n",
        "    \"\"\"\n",
        "    return np.dot(inputs, weights)\n",
        "\n",
        "def step_activation(x):\n",
        "    \"\"\"\n",
        "    Applies the step activation function.\n",
        "\n",
        "    Args:\n",
        "        x: The input value.\n",
        "\n",
        "    Returns:\n",
        "        1 if x >= 0, 0 otherwise.\n",
        "    \"\"\"\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "def bipolar_step_activation(x):\n",
        "    \"\"\"\n",
        "    Applies the bipolar step activation function.\n",
        "\n",
        "    Args:\n",
        "        x: The input value.\n",
        "\n",
        "    Returns:\n",
        "        1 if x > 0, 0 if x == 0, -1 otherwise.\n",
        "    \"\"\"\n",
        "    if x > 0:\n",
        "        return 1\n",
        "    elif x == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def sigmoid_activation(x):\n",
        "    \"\"\"\n",
        "    Applies the sigmoid activation function.\n",
        "\n",
        "    Args:\n",
        "        x: The input value.\n",
        "\n",
        "    Returns:\n",
        "        1 / (1 + np.exp(-x))\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu_activation(x):\n",
        "    \"\"\"\n",
        "    Applies the ReLU activation function.\n",
        "\n",
        "    Args:\n",
        "        x: The input value.\n",
        "\n",
        "    Returns:\n",
        "        max(0, x)\n",
        "    \"\"\"\n",
        "    return max(0, x)\n",
        "\n",
        "def train_perceptron(X, y, activation_func, learning_rate=0.05, max_epochs=1000, error_threshold=0.002):\n",
        "    \"\"\"\n",
        "    Trains a perceptron using the given training data and activation function.\n",
        "\n",
        "    Args:\n",
        "        X: The input data.\n",
        "        y: The target output.\n",
        "        activation_func: The activation function to use.\n",
        "        learning_rate: The learning rate.\n",
        "        max_epochs: The maximum number of epochs.\n",
        "        error_threshold: The error threshold for convergence.\n",
        "\n",
        "    Returns:\n",
        "        The trained perceptron weights and the number of epochs.\n",
        "    \"\"\"\n",
        "    num_features = X.shape[1]\n",
        "    weights = np.random.rand(num_features + 1)  # Initialize weights (including bias)\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(X)):\n",
        "            net_input = summation_unit(X[i], weights[:-1]) + weights[-1]\n",
        "            predicted_output = activation_func(net_input)\n",
        "            error = y[i] - predicted_output\n",
        "            total_error += error ** 2\n",
        "\n",
        "            # Update weights\n",
        "            weights[:-1] += learning_rate * error * X[i]\n",
        "            weights[-1] += learning_rate * error\n",
        "\n",
        "        # Check for convergence\n",
        "        if total_error <= error_threshold:\n",
        "            break\n",
        "\n",
        "    return weights, epoch\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Preprocesses the data by converting text to numerical format using TF-IDF.\n",
        "\n",
        "    Args:\n",
        "        df: A pandas DataFrame containing the English and Hindi columns.\n",
        "\n",
        "    Returns:\n",
        "        X: A numpy array of feature vectors.\n",
        "        y: A numpy array of target values.\n",
        "    \"\"\"\n",
        "    # Check column names\n",
        "    print(\"Columns in DataFrame:\", df.columns.tolist())\n",
        "\n",
        "    # Initialize TF-IDF Vectorizers for English and Hindi columns\n",
        "    tfidf_vectorizer_english = TfidfVectorizer()\n",
        "    tfidf_vectorizer_hindi = TfidfVectorizer()\n",
        "\n",
        "    # Apply TF-IDF Vectorization\n",
        "    X_english = tfidf_vectorizer_english.fit_transform(df['ENGLISH']).toarray()\n",
        "    X_hindi = tfidf_vectorizer_hindi.fit_transform(df['HINDI']).toarray()\n",
        "\n",
        "    # Combine features from both languages\n",
        "    X = np.hstack((X_english, X_hindi))\n",
        "\n",
        "    # Create a synthetic target column (for demonstration purposes)\n",
        "    # Replace this with your actual target column if available\n",
        "    y = np.random.randint(0, 2, size=X.shape[0])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    file_path = 'Book1.xlsx'\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Select only the relevant columns\n",
        "    df = df[['ENGLISH', 'HINDI']]\n",
        "\n",
        "    # Preprocess data\n",
        "    X, y = preprocess_data(df)\n",
        "\n",
        "    # Activation functions to test\n",
        "    activation_funcs = [bipolar_step_activation, sigmoid_activation, relu_activation]\n",
        "    labels = [\"Bipolar Step\", \"Sigmoid\", \"ReLU\"]\n",
        "\n",
        "    # Train perceptron for each activation function\n",
        "    for i, activation_func in enumerate(activation_funcs):\n",
        "        weights, epochs = train_perceptron(X, y, activation_func)\n",
        "        print(f\"Activation Function: {labels[i]}\")\n",
        "        print(f\"Final weights: {weights}\")\n",
        "        print(f\"Number of epochs: {epochs}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7ijvRlgIPyT",
        "outputId": "e04f7372-b69c-43ff-96b1-e886a23a2d77"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame: ['ENGLISH', 'HINDI']\n",
            "Activation Function: Bipolar Step\n",
            "Final weights: [ 0.7290391   0.76199109  0.65022565 ...  0.61015959  0.5917897\n",
            " -0.1225685 ]\n",
            "Number of epochs: 999\n",
            "Activation Function: Sigmoid\n",
            "Final weights: [ 3.24337449  0.9458587   3.99737814 ...  3.54637767  4.25582072\n",
            " -0.79819113]\n",
            "Number of epochs: 999\n",
            "Activation Function: ReLU\n",
            "Final weights: [ 0.19001481 -0.13006622  2.14470497 ...  1.01870014  0.93575042\n",
            " -0.29530486]\n",
            "Number of epochs: 999\n"
          ]
        }
      ]
    }
  ]
}